TASK 1 : STOCK PREDICTION
PURPOSE : TO PREDICT THE STOCK PRICE OF A COMPANY USING LSTM.
ABOUT DATASET
Google Stock Prediction
This dataset contains historical data of Google's stock prices and related attributes. It consists of 14 columns and a smaller subset of 1257 rows. Each column represents a specific attribute, and each row contains the corresponding values for that attribute.

The columns in the dataset are as follows:

Symbol: The name of the company, which is GOOG in this case.
Date: The year and date of the stock data.
Close: The closing price of Google's stock on a particular day.
High: The highest value reached by Google's stock on the given day.
Low: The lowest value reached by Google's stock on the given day.
Open: The opening value of Google's stock on the given day.
Volume: The trading volume of Google's stock on the given day, i.e., the number of shares traded.
adjClose: The adjusted closing price of Google's stock, considering factors such as dividends and stock splits.
adjHigh: The adjusted highest value reached by Google's stock on the given day.
adjLow: The adjusted lowest value reached by Google's stock on the given day.
adjOpen: The adjusted opening value of Google's stock on the given day.
adjVolume: The adjusted trading volume of Google's stock on the given day, accounting for factors such as stock splits.
divCash: The amount of cash dividend paid out to shareholders on the given day.
splitFactor: The split factor, if any, applied to Google's stock on the given day. A split factor of 1 indicates no split.
The dataset is available at Kaggle : https://www.kaggle.com/datasets/shreenidhihipparagi/google-stock-prediction

STEPS INVOLVED :
1 . IMPORTING LIBRARIES AND DATA TO BE USED
2. GATHERING INSIGHTS
3. DATA PRE-PROCESSING
4. CREATING LSTM MODEL
5. VISUALIZING ACTUAL VS PREDICTED DATA
6. PREDICTING UPCOMING 15 DAYS


STEP 1 : IMPORTING LIBRARIES AND DATA TO BE USED
#importing libraries to be used
import numpy as np # for linear algebra
import pandas as pd # data preprocessing
import matplotlib.pyplot as plt # data visualization library
import seaborn as sns # data visualization library
%matplotlib inline
import warnings
warnings.filterwarnings('ignore') # ignore warnings 

from sklearn.preprocessing import MinMaxScaler # for normalization
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, Bidirectional
df = pd.read_csv('Google Stocks.csv') # data_importing
df.head(10) # fetching first 10 rows of dataset
symbol	date	close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
0	GOOG	2016-06-14 00:00:00+00:00	718.27	722.47	713.1200	716.48	1306065	718.27	722.47	713.1200	716.48	1306065	0.0	1.0
1	GOOG	2016-06-15 00:00:00+00:00	718.92	722.98	717.3100	719.00	1214517	718.92	722.98	717.3100	719.00	1214517	0.0	1.0
2	GOOG	2016-06-16 00:00:00+00:00	710.36	716.65	703.2600	714.91	1982471	710.36	716.65	703.2600	714.91	1982471	0.0	1.0
3	GOOG	2016-06-17 00:00:00+00:00	691.72	708.82	688.4515	708.65	3402357	691.72	708.82	688.4515	708.65	3402357	0.0	1.0
4	GOOG	2016-06-20 00:00:00+00:00	693.71	702.48	693.4100	698.77	2082538	693.71	702.48	693.4100	698.77	2082538	0.0	1.0
5	GOOG	2016-06-21 00:00:00+00:00	695.94	702.77	692.0100	698.40	1465634	695.94	702.77	692.0100	698.40	1465634	0.0	1.0
6	GOOG	2016-06-22 00:00:00+00:00	697.46	700.86	693.0819	699.06	1184318	697.46	700.86	693.0819	699.06	1184318	0.0	1.0
7	GOOG	2016-06-23 00:00:00+00:00	701.87	701.95	687.0000	697.45	2171415	701.87	701.95	687.0000	697.45	2171415	0.0	1.0
8	GOOG	2016-06-24 00:00:00+00:00	675.22	689.40	673.4500	675.17	4449022	675.22	689.40	673.4500	675.17	4449022	0.0	1.0
9	GOOG	2016-06-27 00:00:00+00:00	668.26	672.30	663.2840	671.00	2641085	668.26	672.30	663.2840	671.00	2641085	0.0	1.0
STEP 2 : GATHERING INSIGHTS
# shape of data
print("Shape of data:",df.shape)
Shape of data: (1258, 14)
# statistical description of data
df.describe()
close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
count	1258.000000	1258.000000	1258.000000	1258.000000	1.258000e+03	1258.000000	1258.000000	1258.000000	1258.000000	1.258000e+03	1258.0	1258.0
mean	1216.317067	1227.430934	1204.176430	1215.260779	1.601590e+06	1216.317067	1227.430936	1204.176436	1215.260779	1.601590e+06	0.0	1.0
std	383.333358	387.570872	378.777094	382.446995	6.960172e+05	383.333358	387.570873	378.777099	382.446995	6.960172e+05	0.0	0.0
min	668.260000	672.300000	663.284000	671.000000	3.467530e+05	668.260000	672.300000	663.284000	671.000000	3.467530e+05	0.0	1.0
25%	960.802500	968.757500	952.182500	959.005000	1.173522e+06	960.802500	968.757500	952.182500	959.005000	1.173522e+06	0.0	1.0
50%	1132.460000	1143.935000	1117.915000	1131.150000	1.412588e+06	1132.460000	1143.935000	1117.915000	1131.150000	1.412588e+06	0.0	1.0
75%	1360.595000	1374.345000	1348.557500	1361.075000	1.812156e+06	1360.595000	1374.345000	1348.557500	1361.075000	1.812156e+06	0.0	1.0
max	2521.600000	2526.990000	2498.290000	2524.920000	6.207027e+06	2521.600000	2526.990000	2498.290000	2524.920000	6.207027e+06	0.0	1.0
# summary of data
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1258 entries, 0 to 1257
Data columns (total 14 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   symbol       1258 non-null   object 
 1   date         1258 non-null   object 
 2   close        1258 non-null   float64
 3   high         1258 non-null   float64
 4   low          1258 non-null   float64
 5   open         1258 non-null   float64
 6   volume       1258 non-null   int64  
 7   adjClose     1258 non-null   float64
 8   adjHigh      1258 non-null   float64
 9   adjLow       1258 non-null   float64
 10  adjOpen      1258 non-null   float64
 11  adjVolume    1258 non-null   int64  
 12  divCash      1258 non-null   float64
 13  splitFactor  1258 non-null   float64
dtypes: float64(10), int64(2), object(2)
memory usage: 137.7+ KB
# checking null values
df.isnull().sum()
symbol         0
date           0
close          0
high           0
low            0
open           0
volume         0
adjClose       0
adjHigh        0
adjLow         0
adjOpen        0
adjVolume      0
divCash        0
splitFactor    0
dtype: int64
There are no null values in the dataset
df = df[['date','open','close']] # Extracting required columns
df['date'] = pd.to_datetime(df['date'].apply(lambda x: x.split()[0])) # converting object dtype of date column to datetime dtype
df.set_index('date',drop=True,inplace=True) # Setting date column as index
df.head(10)
open	close
date		
2016-06-14	716.48	718.27
2016-06-15	719.00	718.92
2016-06-16	714.91	710.36
2016-06-17	708.65	691.72
2016-06-20	698.77	693.71
2016-06-21	698.40	695.94
2016-06-22	699.06	697.46
2016-06-23	697.45	701.87
2016-06-24	675.17	675.22
2016-06-27	671.00	668.26
# plotting open and closing price on date index
fig, ax =plt.subplots(1,2,figsize=(20,7))
ax[0].plot(df['open'],label='Open',color='green')
ax[0].set_xlabel('Date',size=15)
ax[0].set_ylabel('Price',size=15)
ax[0].legend()

ax[1].plot(df['close'],label='Close',color='red')
ax[1].set_xlabel('Date',size=15)
ax[1].set_ylabel('Price',size=15)
ax[1].legend()

fig.show()


STEP 3 : DATA PRE-PROCESSING
# normalizing all the values of all columns using MinMaxScaler
MMS = MinMaxScaler()
df[df.columns] = MMS.fit_transform(df)
df.head(10)
open	close
date		
2016-06-14	0.024532	0.026984
2016-06-15	0.025891	0.027334
2016-06-16	0.023685	0.022716
2016-06-17	0.020308	0.012658
2016-06-20	0.014979	0.013732
2016-06-21	0.014779	0.014935
2016-06-22	0.015135	0.015755
2016-06-23	0.014267	0.018135
2016-06-24	0.002249	0.003755
2016-06-27	0.000000	0.000000
# splitting the data into training and test set
training_size = round(len(df) * 0.75) # Selecting 75 % for training and 25 % for testing
training_size
944
train_data = df[:training_size]
test_data  = df[training_size:]

train_data.shape, test_data.shape
((944, 2), (314, 2))
# Function to create sequence of data for training and testing

def create_sequence(dataset):
  sequences = []
  labels = []

  start_idx = 0

  for stop_idx in range(50,len(dataset)): # Selecting 50 rows at a time
    sequences.append(dataset.iloc[start_idx:stop_idx])
    labels.append(dataset.iloc[stop_idx])
    start_idx += 1
  return (np.array(sequences),np.array(labels))
train_seq, train_label = create_sequence(train_data) 
test_seq, test_label = create_sequence(test_data)
train_seq.shape, train_label.shape, test_seq.shape, test_label.shape
((894, 50, 2), (894, 2), (264, 50, 2), (264, 2))
STEP 4 : CREATING LSTM MODEL
# imported Sequential from keras.models 
model = Sequential()
# importing Dense, Dropout, LSTM, Bidirectional from keras.layers 
model.add(LSTM(units=50, return_sequences=True, input_shape = (train_seq.shape[1], train_seq.shape[2])))

model.add(Dropout(0.1)) 
model.add(LSTM(units=50))

model.add(Dense(2))

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])

model.summary()
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_2 (LSTM)               (None, 50, 50)            10600     
                                                                 
 dropout_1 (Dropout)         (None, 50, 50)            0         
                                                                 
 lstm_3 (LSTM)               (None, 50)                20200     
                                                                 
 dense_1 (Dense)             (None, 2)                 102       
                                                                 
=================================================================
Total params: 30,902
Trainable params: 30,902
Non-trainable params: 0
_________________________________________________________________
# fitting the model by iterating the dataset over 100 times(100 epochs)
model.fit(train_seq, train_label, epochs=100,validation_data=(test_seq, test_label), verbose=1)
Epoch 1/100
28/28 [==============================] - 6s 76ms/step - loss: 0.0075 - mean_absolute_error: 0.0615 - val_loss: 0.0177 - val_mean_absolute_error: 0.1109
Epoch 2/100
28/28 [==============================] - 1s 36ms/step - loss: 7.4780e-04 - mean_absolute_error: 0.0213 - val_loss: 0.0036 - val_mean_absolute_error: 0.0456
Epoch 3/100
28/28 [==============================] - 1s 37ms/step - loss: 5.1931e-04 - mean_absolute_error: 0.0167 - val_loss: 0.0063 - val_mean_absolute_error: 0.0636
Epoch 4/100
28/28 [==============================] - 1s 42ms/step - loss: 4.8920e-04 - mean_absolute_error: 0.0166 - val_loss: 0.0051 - val_mean_absolute_error: 0.0564
Epoch 5/100
28/28 [==============================] - 1s 34ms/step - loss: 4.5922e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0068 - val_mean_absolute_error: 0.0673
Epoch 6/100
28/28 [==============================] - 1s 32ms/step - loss: 4.8714e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0041 - val_mean_absolute_error: 0.0488
Epoch 7/100
28/28 [==============================] - 1s 33ms/step - loss: 4.3301e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0038 - val_mean_absolute_error: 0.0476
Epoch 8/100
28/28 [==============================] - 1s 34ms/step - loss: 4.2331e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0051 - val_mean_absolute_error: 0.0563
Epoch 9/100
28/28 [==============================] - 1s 34ms/step - loss: 4.1519e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0037 - val_mean_absolute_error: 0.0467
Epoch 10/100
28/28 [==============================] - 1s 32ms/step - loss: 3.8853e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0037 - val_mean_absolute_error: 0.0463
Epoch 11/100
28/28 [==============================] - 1s 34ms/step - loss: 4.1915e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0043 - val_mean_absolute_error: 0.0507
Epoch 12/100
28/28 [==============================] - 1s 37ms/step - loss: 4.3963e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0042 - val_mean_absolute_error: 0.0509
Epoch 13/100
28/28 [==============================] - 1s 47ms/step - loss: 3.5439e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0040 - val_mean_absolute_error: 0.0491
Epoch 14/100
28/28 [==============================] - 1s 35ms/step - loss: 3.5638e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0042 - val_mean_absolute_error: 0.0505
Epoch 15/100
28/28 [==============================] - 1s 34ms/step - loss: 3.2939e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0032 - val_mean_absolute_error: 0.0436
Epoch 16/100
28/28 [==============================] - 1s 33ms/step - loss: 3.4633e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0027 - val_mean_absolute_error: 0.0390
Epoch 17/100
28/28 [==============================] - 1s 33ms/step - loss: 3.3274e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0061 - val_mean_absolute_error: 0.0638
Epoch 18/100
28/28 [==============================] - 1s 33ms/step - loss: 3.3437e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0036 - val_mean_absolute_error: 0.0475
Epoch 19/100
28/28 [==============================] - 1s 33ms/step - loss: 3.2053e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0039 - val_mean_absolute_error: 0.0500
Epoch 20/100
28/28 [==============================] - 1s 33ms/step - loss: 3.0536e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0066 - val_mean_absolute_error: 0.0676
Epoch 21/100
28/28 [==============================] - 1s 33ms/step - loss: 3.1899e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0037 - val_mean_absolute_error: 0.0475
Epoch 22/100
28/28 [==============================] - 1s 33ms/step - loss: 3.0243e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0042 - val_mean_absolute_error: 0.0517
Epoch 23/100
28/28 [==============================] - 1s 34ms/step - loss: 2.8721e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0038 - val_mean_absolute_error: 0.0481
Epoch 24/100
28/28 [==============================] - 1s 36ms/step - loss: 2.8524e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0047 - val_mean_absolute_error: 0.0556
Epoch 25/100
28/28 [==============================] - 1s 34ms/step - loss: 2.9709e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0020 - val_mean_absolute_error: 0.0329
Epoch 26/100
28/28 [==============================] - 1s 32ms/step - loss: 2.8813e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0039 - val_mean_absolute_error: 0.0494
Epoch 27/100
28/28 [==============================] - 1s 33ms/step - loss: 3.0037e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0072 - val_mean_absolute_error: 0.0716
Epoch 28/100
28/28 [==============================] - 1s 33ms/step - loss: 2.9664e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0056 - val_mean_absolute_error: 0.0618
Epoch 29/100
28/28 [==============================] - 1s 33ms/step - loss: 2.9759e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0014 - val_mean_absolute_error: 0.0276
Epoch 30/100
28/28 [==============================] - 1s 32ms/step - loss: 3.4480e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0011 - val_mean_absolute_error: 0.0246
Epoch 31/100
28/28 [==============================] - 1s 32ms/step - loss: 2.6217e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0023 - val_mean_absolute_error: 0.0377
Epoch 32/100
28/28 [==============================] - 1s 32ms/step - loss: 2.4907e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0026 - val_mean_absolute_error: 0.0407
Epoch 33/100
28/28 [==============================] - 1s 32ms/step - loss: 2.7553e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0054 - val_mean_absolute_error: 0.0621
Epoch 34/100
28/28 [==============================] - 1s 33ms/step - loss: 2.8729e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0035 - val_mean_absolute_error: 0.0476
Epoch 35/100
28/28 [==============================] - 1s 32ms/step - loss: 2.7106e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0032 - val_mean_absolute_error: 0.0434
Epoch 36/100
28/28 [==============================] - 1s 32ms/step - loss: 2.3782e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0027 - val_mean_absolute_error: 0.0398
Epoch 37/100
28/28 [==============================] - 1s 34ms/step - loss: 2.2473e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0026 - val_mean_absolute_error: 0.0394
Epoch 38/100
28/28 [==============================] - 1s 33ms/step - loss: 2.2428e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0024 - val_mean_absolute_error: 0.0381
Epoch 39/100
28/28 [==============================] - 1s 32ms/step - loss: 2.2204e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0035 - val_mean_absolute_error: 0.0477
Epoch 40/100
28/28 [==============================] - 1s 39ms/step - loss: 2.4818e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0024 - val_mean_absolute_error: 0.0392
Epoch 41/100
28/28 [==============================] - 1s 33ms/step - loss: 2.4285e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0025 - val_mean_absolute_error: 0.0393
Epoch 42/100
28/28 [==============================] - 1s 37ms/step - loss: 2.2973e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0018 - val_mean_absolute_error: 0.0327
Epoch 43/100
28/28 [==============================] - 1s 35ms/step - loss: 2.6763e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0021 - val_mean_absolute_error: 0.0345
Epoch 44/100
28/28 [==============================] - 1s 32ms/step - loss: 2.1582e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0020 - val_mean_absolute_error: 0.0350
Epoch 45/100
28/28 [==============================] - 1s 32ms/step - loss: 2.2262e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0024 - val_mean_absolute_error: 0.0388
Epoch 46/100
28/28 [==============================] - 1s 32ms/step - loss: 2.1803e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0017 - val_mean_absolute_error: 0.0311
Epoch 47/100
28/28 [==============================] - 1s 31ms/step - loss: 1.9663e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0016 - val_mean_absolute_error: 0.0301
Epoch 48/100
28/28 [==============================] - 1s 33ms/step - loss: 2.1869e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0020 - val_mean_absolute_error: 0.0340
Epoch 49/100
28/28 [==============================] - 1s 33ms/step - loss: 2.0821e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0029 - val_mean_absolute_error: 0.0432
Epoch 50/100
28/28 [==============================] - 1s 33ms/step - loss: 2.3735e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0023 - val_mean_absolute_error: 0.0373
Epoch 51/100
28/28 [==============================] - 1s 32ms/step - loss: 1.9933e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0022 - val_mean_absolute_error: 0.0362
Epoch 52/100
28/28 [==============================] - 1s 34ms/step - loss: 2.0226e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0014 - val_mean_absolute_error: 0.0280
Epoch 53/100
28/28 [==============================] - 1s 32ms/step - loss: 2.1465e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0019 - val_mean_absolute_error: 0.0339
Epoch 54/100
28/28 [==============================] - 1s 33ms/step - loss: 1.9009e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0028 - val_mean_absolute_error: 0.0428
Epoch 55/100
28/28 [==============================] - 1s 32ms/step - loss: 2.0381e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0015 - val_mean_absolute_error: 0.0290
Epoch 56/100
28/28 [==============================] - 1s 32ms/step - loss: 1.8664e-04 - mean_absolute_error: 0.0099 - val_loss: 0.0019 - val_mean_absolute_error: 0.0345
Epoch 57/100
28/28 [==============================] - 1s 32ms/step - loss: 1.7744e-04 - mean_absolute_error: 0.0096 - val_loss: 0.0015 - val_mean_absolute_error: 0.0293
Epoch 58/100
28/28 [==============================] - 1s 32ms/step - loss: 1.6983e-04 - mean_absolute_error: 0.0093 - val_loss: 0.0015 - val_mean_absolute_error: 0.0302
Epoch 59/100
28/28 [==============================] - 1s 35ms/step - loss: 1.7348e-04 - mean_absolute_error: 0.0096 - val_loss: 0.0010 - val_mean_absolute_error: 0.0238
Epoch 60/100
28/28 [==============================] - 1s 34ms/step - loss: 2.0711e-04 - mean_absolute_error: 0.0107 - val_loss: 6.9389e-04 - val_mean_absolute_error: 0.0199
Epoch 61/100
28/28 [==============================] - 1s 32ms/step - loss: 2.0330e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0036 - val_mean_absolute_error: 0.0502
Epoch 62/100
28/28 [==============================] - 1s 32ms/step - loss: 1.7130e-04 - mean_absolute_error: 0.0095 - val_loss: 0.0023 - val_mean_absolute_error: 0.0377
Epoch 63/100
28/28 [==============================] - 1s 32ms/step - loss: 1.7350e-04 - mean_absolute_error: 0.0095 - val_loss: 0.0012 - val_mean_absolute_error: 0.0262
Epoch 64/100
28/28 [==============================] - 1s 32ms/step - loss: 1.7681e-04 - mean_absolute_error: 0.0097 - val_loss: 0.0017 - val_mean_absolute_error: 0.0324
Epoch 65/100
28/28 [==============================] - 1s 34ms/step - loss: 1.9607e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0019 - val_mean_absolute_error: 0.0348
Epoch 66/100
28/28 [==============================] - 1s 33ms/step - loss: 1.7603e-04 - mean_absolute_error: 0.0098 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252
Epoch 67/100
28/28 [==============================] - 1s 35ms/step - loss: 1.6057e-04 - mean_absolute_error: 0.0092 - val_loss: 0.0013 - val_mean_absolute_error: 0.0281
Epoch 68/100
28/28 [==============================] - 1s 33ms/step - loss: 1.6818e-04 - mean_absolute_error: 0.0094 - val_loss: 0.0014 - val_mean_absolute_error: 0.0293
Epoch 69/100
28/28 [==============================] - 1s 34ms/step - loss: 1.5278e-04 - mean_absolute_error: 0.0089 - val_loss: 0.0027 - val_mean_absolute_error: 0.0433
Epoch 70/100
28/28 [==============================] - 1s 33ms/step - loss: 1.6069e-04 - mean_absolute_error: 0.0093 - val_loss: 8.7465e-04 - val_mean_absolute_error: 0.0216
Epoch 71/100
28/28 [==============================] - 1s 32ms/step - loss: 1.5866e-04 - mean_absolute_error: 0.0092 - val_loss: 0.0010 - val_mean_absolute_error: 0.0235
Epoch 72/100
28/28 [==============================] - 1s 33ms/step - loss: 1.5660e-04 - mean_absolute_error: 0.0090 - val_loss: 0.0014 - val_mean_absolute_error: 0.0298
Epoch 73/100
28/28 [==============================] - 1s 32ms/step - loss: 1.6442e-04 - mean_absolute_error: 0.0093 - val_loss: 5.9834e-04 - val_mean_absolute_error: 0.0180
Epoch 74/100
28/28 [==============================] - 1s 32ms/step - loss: 1.5159e-04 - mean_absolute_error: 0.0088 - val_loss: 7.7283e-04 - val_mean_absolute_error: 0.0202
Epoch 75/100
28/28 [==============================] - 1s 33ms/step - loss: 1.5390e-04 - mean_absolute_error: 0.0090 - val_loss: 0.0016 - val_mean_absolute_error: 0.0320
Epoch 76/100
28/28 [==============================] - 1s 33ms/step - loss: 1.4731e-04 - mean_absolute_error: 0.0087 - val_loss: 0.0013 - val_mean_absolute_error: 0.0278
Epoch 77/100
28/28 [==============================] - 1s 34ms/step - loss: 1.4074e-04 - mean_absolute_error: 0.0086 - val_loss: 0.0015 - val_mean_absolute_error: 0.0320
Epoch 78/100
28/28 [==============================] - 1s 33ms/step - loss: 1.5305e-04 - mean_absolute_error: 0.0090 - val_loss: 0.0011 - val_mean_absolute_error: 0.0244
Epoch 79/100
28/28 [==============================] - 1s 33ms/step - loss: 1.4120e-04 - mean_absolute_error: 0.0088 - val_loss: 7.8814e-04 - val_mean_absolute_error: 0.0207
Epoch 80/100
28/28 [==============================] - 1s 33ms/step - loss: 1.6674e-04 - mean_absolute_error: 0.0094 - val_loss: 5.3724e-04 - val_mean_absolute_error: 0.0170
Epoch 81/100
28/28 [==============================] - 1s 34ms/step - loss: 1.7465e-04 - mean_absolute_error: 0.0097 - val_loss: 9.3549e-04 - val_mean_absolute_error: 0.0240
Epoch 82/100
28/28 [==============================] - 1s 32ms/step - loss: 1.4862e-04 - mean_absolute_error: 0.0088 - val_loss: 0.0013 - val_mean_absolute_error: 0.0280
Epoch 83/100
28/28 [==============================] - 1s 34ms/step - loss: 1.3837e-04 - mean_absolute_error: 0.0084 - val_loss: 9.0192e-04 - val_mean_absolute_error: 0.0227
Epoch 84/100
28/28 [==============================] - 1s 32ms/step - loss: 1.2769e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0017 - val_mean_absolute_error: 0.0328
Epoch 85/100
28/28 [==============================] - 1s 34ms/step - loss: 1.4004e-04 - mean_absolute_error: 0.0086 - val_loss: 8.2948e-04 - val_mean_absolute_error: 0.0210
Epoch 86/100
28/28 [==============================] - 1s 32ms/step - loss: 1.4636e-04 - mean_absolute_error: 0.0089 - val_loss: 8.2554e-04 - val_mean_absolute_error: 0.0213
Epoch 87/100
28/28 [==============================] - 1s 32ms/step - loss: 1.2793e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0016 - val_mean_absolute_error: 0.0319
Epoch 88/100
28/28 [==============================] - 1s 33ms/step - loss: 1.4175e-04 - mean_absolute_error: 0.0086 - val_loss: 5.3922e-04 - val_mean_absolute_error: 0.0172
Epoch 89/100
28/28 [==============================] - 1s 34ms/step - loss: 1.3783e-04 - mean_absolute_error: 0.0084 - val_loss: 6.6209e-04 - val_mean_absolute_error: 0.0185
Epoch 90/100
28/28 [==============================] - 1s 32ms/step - loss: 1.2677e-04 - mean_absolute_error: 0.0081 - val_loss: 0.0017 - val_mean_absolute_error: 0.0332
Epoch 91/100
28/28 [==============================] - 1s 33ms/step - loss: 1.2505e-04 - mean_absolute_error: 0.0082 - val_loss: 5.2412e-04 - val_mean_absolute_error: 0.0167
Epoch 92/100
28/28 [==============================] - 1s 32ms/step - loss: 1.2061e-04 - mean_absolute_error: 0.0079 - val_loss: 8.0274e-04 - val_mean_absolute_error: 0.0213
Epoch 93/100
28/28 [==============================] - 1s 34ms/step - loss: 1.2418e-04 - mean_absolute_error: 0.0080 - val_loss: 0.0021 - val_mean_absolute_error: 0.0386
Epoch 94/100
28/28 [==============================] - 1s 39ms/step - loss: 1.3460e-04 - mean_absolute_error: 0.0086 - val_loss: 6.5789e-04 - val_mean_absolute_error: 0.0188
Epoch 95/100
28/28 [==============================] - 1s 35ms/step - loss: 1.2995e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0012 - val_mean_absolute_error: 0.0274
Epoch 96/100
28/28 [==============================] - 1s 36ms/step - loss: 1.4096e-04 - mean_absolute_error: 0.0086 - val_loss: 0.0012 - val_mean_absolute_error: 0.0272
Epoch 97/100
28/28 [==============================] - 1s 37ms/step - loss: 1.3420e-04 - mean_absolute_error: 0.0085 - val_loss: 0.0016 - val_mean_absolute_error: 0.0331
Epoch 98/100
28/28 [==============================] - 1s 33ms/step - loss: 1.2673e-04 - mean_absolute_error: 0.0081 - val_loss: 5.2320e-04 - val_mean_absolute_error: 0.0169
Epoch 99/100
28/28 [==============================] - 1s 33ms/step - loss: 1.1939e-04 - mean_absolute_error: 0.0078 - val_loss: 7.9773e-04 - val_mean_absolute_error: 0.0201
Epoch 100/100
28/28 [==============================] - 1s 33ms/step - loss: 1.2219e-04 - mean_absolute_error: 0.0079 - val_loss: 0.0012 - val_mean_absolute_error: 0.0266
<keras.callbacks.History at 0x1d101665a30>
# predicting the values after running the model
test_predicted = model.predict(test_seq)
test_predicted[:5]
9/9 [==============================] - 1s 10ms/step
array([[0.40134126, 0.39790046],
       [0.4016015 , 0.39767465],
       [0.397627  , 0.3933278 ],
       [0.39950964, 0.39557028],
       [0.402757  , 0.39912042]], dtype=float32)
# Inversing normalization/scaling on predicted data 
test_inverse_predicted = MMS.inverse_transform(test_predicted)
test_inverse_predicted[:5]
array([[1415.0546, 1405.7048],
       [1415.5371, 1405.2864],
       [1408.1687, 1397.2301],
       [1411.6588, 1401.3862],
       [1417.6792, 1407.9658]], dtype=float32)
STEP 5 : VISUALIZING ACTUAL VS PREDICTED DATA
# Merging actual and predicted data for better visualization
df_merge = pd.concat([df.iloc[-264:].copy(),
                          pd.DataFrame(test_inverse_predicted,columns=['open_predicted','close_predicted'],
                                       index=df.iloc[-264:].index)], axis=1)
# Inversing normalization/scaling 
df_merge[['open','close']] = MMS.inverse_transform(df_merge[['open','close']])
df_merge.head()
open	close	open_predicted	close_predicted
date				
2020-05-27	1417.25	1417.84	1415.054565	1405.704834
2020-05-28	1396.86	1416.73	1415.537109	1405.286377
2020-05-29	1416.94	1428.92	1408.168701	1397.230103
2020-06-01	1418.39	1431.82	1411.658813	1401.386230
2020-06-02	1430.55	1439.22	1417.679199	1407.965820
# plotting the actual open and predicted open prices on date index
df_merge[['open','open_predicted']].plot(figsize=(10,6))
plt.xticks(rotation=45)
plt.xlabel('Date',size=15)
plt.ylabel('Stock Price',size=15)
plt.title('Actual vs Predicted for open price',size=15)
plt.show()

# plotting the actual close and predicted close prices on date index 
df_merge[['close','close_predicted']].plot(figsize=(10,6))
plt.xticks(rotation=45)
plt.xlabel('Date',size=15)
plt.ylabel('Stock Price',size=15)
plt.title('Actual vs Predicted for close price',size=15)
plt.show()

STEP 6. PREDICTING UPCOMING 10 DAYS
# Creating a dataframe and adding 10 days to existing index 

df_merge = df_merge.append(pd.DataFrame(columns=df_merge.columns,
                                        index=pd.date_range(start=df_merge.index[-1], periods=11, freq='D', closed='right')))
df_merge['2021-06-09':'2021-06-16']
open	close	open_predicted	close_predicted
2021-06-09	2499.50	2491.40	2419.937012	2351.718994
2021-06-10	2494.01	2521.60	2429.809814	2357.459229
2021-06-11	2524.92	2513.93	2435.215576	2360.057861
2021-06-12	NaN	NaN	NaN	NaN
2021-06-13	NaN	NaN	NaN	NaN
2021-06-14	NaN	NaN	NaN	NaN
2021-06-15	NaN	NaN	NaN	NaN
2021-06-16	NaN	NaN	NaN	NaN
# creating a DataFrame and filling values of open and close column
upcoming_prediction = pd.DataFrame(columns=['open','close'],index=df_merge.index)
upcoming_prediction.index=pd.to_datetime(upcoming_prediction.index)
curr_seq = test_seq[-1:]

for i in range(-10,0):
  up_pred = model.predict(curr_seq)
  upcoming_prediction.iloc[i] = up_pred
  curr_seq = np.append(curr_seq[0][1:],up_pred,axis=0)
  curr_seq = curr_seq.reshape(test_seq[-1:].shape)
1/1 [==============================] - 0s 36ms/step
1/1 [==============================] - 0s 23ms/step
1/1 [==============================] - 0s 30ms/step
1/1 [==============================] - 0s 24ms/step
1/1 [==============================] - 0s 20ms/step
1/1 [==============================] - 0s 21ms/step
1/1 [==============================] - 0s 24ms/step
1/1 [==============================] - 0s 31ms/step
1/1 [==============================] - 0s 26ms/step
1/1 [==============================] - 0s 30ms/step
# inversing Normalization/scaling
upcoming_prediction[['open','close']] = MMS.inverse_transform(upcoming_prediction[['open','close']])
# plotting Upcoming Open price on date index
fig,ax=plt.subplots(figsize=(10,5))
ax.plot(df_merge.loc['2021-04-01':,'open'],label='Current Open Price')
ax.plot(upcoming_prediction.loc['2021-04-01':,'open'],label='Upcoming Open Price')
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
ax.set_xlabel('Date',size=15)
ax.set_ylabel('Stock Price',size=15)
ax.set_title('Upcoming Open price prediction',size=15)
ax.legend()
fig.show()

# plotting Upcoming Close price on date index
fig,ax=plt.subplots(figsize=(10,5))
ax.plot(df_merge.loc['2021-04-01':,'close'],label='Current close Price')
ax.plot(upcoming_prediction.loc['2021-04-01':,'close'],label='Upcoming close Price')
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
ax.set_xlabel('Date',size=15)
ax.set_ylabel('Stock Price',size=15)
ax.set_title('Upcoming close price prediction',size=15)
ax.legend()
fig.show()

THANK YOU!
 


